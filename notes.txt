lstm1:
model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.Dense(2, activation='softmax')
])

batches = 10
n = 10
epochs = 30
patience = 10

Mean Jaccard Index = 0.07
f1-score = 0.21
Precision = 0.62
Recall = 0.10


Notes: baseline model used to troubleshoot infrastructure code. Logging facility added, required info copied to s3 after model run, fault-tolerant
data-loading, parallelized loading & preprocessing, etc. Baseline model is overfitting. Model is probably not powerful enough - try adding
additional layers.


lstm2:
model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.Dense(2, activation='softmax')
])

batches = 10
n = 10
epochs = 30
patience = 10

loss: 0.2739 - accuracy: 0.8888 - val_loss: 0.2970 - val_accuracy: 0.8813

val_size = 30,000

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00

Notes: lstm2 guesses negative class for every sample. Training and validation accuracy are approximately equal, so it's not a matter of overfitting
. Hypothesis is that the model is overly regularized. Try reducing regularization in next model.


ltsm3:
model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.5),
    keras.layers.Dense(2, activation='softmax')
])

Metrics after batch 8:
INFO:root:Mean Jaccard Index = 0.11
INFO:root:TP = 580
INFO:root:FP = 970
INFO:root:FN = 2980
INFO:root:f1-score = 0.29
INFO:root:Precision = 0.37
INFO:root:Recall = 0.16


Metrics after batch 9 (final batch):
INFO:root:Mean Jaccard Index = 0.01
INFO:root:TP = 43
INFO:root:FP = 49
INFO:root:FN = 3517
INFO:root:f1-score = 0.02
INFO:root:Precision = 0.47
INFO:root:Recall = 0.01


Notes: model overfits severely on first two batches (predicts all zeros). After batch 4, model is no longer predicting all zeros. This model is
overfitting again, so try increaing regularization slightly. What causes the model to get worse with additional data?

lstm4:
l2_reg = 0.05

model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.2747 - accuracy: 0.8888 - val_loss: 0.2866 - val_accuracy: 0.8813

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00


lstm5:

l2_reg = 0.03

model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.Dense(2, activation='softmax')
])

INFO:root:Mean Jaccard Index = 0.02
INFO:root:TP = 85
INFO:root:FP = 51
INFO:root:FN = 3475
INFO:root:f1-score = 0.05
INFO:root:Precision = 0.62
INFO:root:Recall = 0.02


lstm6:
l2_reg = 0.03

model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.2349 - accuracy: 0.9556 - val_loss: 0.7976 - val_accuracy: 0.8446

INFO:root:Mean Jaccard Index = 0.02
INFO:root:TP = 170
INFO:root:FP = 1271
INFO:root:FN = 3390
INFO:root:f1-score = 0.07
INFO:root:Precision = 0.12
INFO:root:Recall = 0.05

Notes: works much better than multi-layer LSTM, but overfitting. Try increasing regularization. Also try single-layer LSTM.


rnn2:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.2321 - accuracy: 0.9413 - val_loss: 0.6499 - val_accuracy: 0.8697

INFO:root:Mean Jaccard Index = 0.06
INFO:root:TP = 290
INFO:root:FP = 639
INFO:root:FN = 3270
INFO:root:f1-score = 0.15
INFO:root:Precision = 0.31
INFO:root:Recall = 0.08

Notes: increasing regularization has helped slightly. Increase regularization a little more.

rnn3:
alpha = 0.1

model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.2745 - accuracy: 0.9466 - val_loss: 0.6988 - val_accuracy: 0.8744

INFO:root:Mean Jaccard Index = 0.02
INFO:root:TP = 80
INFO:root:FP = 286
INFO:root:FN = 3480
INFO:root:f1-score = 0.04
INFO:root:Precision = 0.22
INFO:root:Recall = 0.02

Notes: could try more epochs with this particular model. Try adding dense layers before recurrent layer. Successive calls to fit will incrementally
 train the model: https://github.com/keras-team/keras/issues/4446

rnn4:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(100, kernel_regularizer=l2(alpha)),
    keras.layers.Dropout(0.2),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(200, kernel_regularizer=l2(alpha)),
    keras.layers.Dropout(0.2),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(250, kernel_regularizer=l2(alpha)),
    keras.layers.Dropout(0.2),
    keras.layers.BatchNormalization(),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.3113 - accuracy: 0.9160 - val_loss: 0.4943 - val_accuracy: 0.8813

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00


rnn5:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(100, kernel_regularizer=l2(alpha)),
    keras.layers.Dropout(0.2),
    keras.layers.BatchNormalization(),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.2158 - accuracy: 0.9500 - val_loss: 0.4441 - val_accuracy: 0.8897

INFO:root:Mean Jaccard Index = 0.10
INFO:root:TP = 444
INFO:root:FP = 193
INFO:root:FN = 3116
INFO:root:f1-score = 0.27
INFO:root:Precision = 0.70
INFO:root:Recall = 0.12

Notes: try 1-convolution


rnn6:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Input(shape=[None, 10057, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=50, kernel_size=5, strides=10,
                        kernel_regularizer=l2(alpha), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.TimeDistributed(keras.layers.Flatten()),
    keras.layers.TimeDistributed(keras.layers.Dense(100, kernel_regularizer=l2(alpha))),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.3147 - accuracy: 0.8910 - val_loss: 0.3319 - val_accuracy: 0.8813

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00

notes: model not learning anything. Try reducing size of convolutional filters, reducing stride, and adding additional convolutional and pooling
layers.

convrnn2:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Input(shape=[None, 10057, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=50, kernel_size=10, strides=2,
                            kernel_regularizer=l2(alpha), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.TimeDistributed(keras.layers.Flatten()),
    keras.layers.TimeDistributed(keras.layers.Dense(100, kernel_regularizer=l2(alpha))),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

Results after training on 5 batches:
INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00

Notes: add batch normalization layers, add activation function to dense layer

convrnn3:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Input(shape=[None, 10057, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=50, kernel_size=10, strides=2,
                            kernel_regularizer=l2(alpha), activation='relu')
    ),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.TimeDistributed(keras.layers.Flatten()),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(keras.layers.Dense(100, kernel_regularizer=l2(alpha), activation='relu')),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

Metrics after first batch:
INFO:root:Mean Jaccard Index = 0.05
INFO:root:TP = 643
INFO:root:FP = 3295
INFO:root:FN = 2917
INFO:root:f1-score = 0.21
INFO:root:Precision = 0.16
INFO:root:Recall = 0.18

loss: 0.3924 - accuracy: 0.9288 - val_loss: 0.5722 - val_accuracy: 0.8753

INFO:root:Mean Jaccard Index = 0.02
INFO:root:TP = 30
INFO:root:FP = 210
INFO:root:FN = 3530
INFO:root:f1-score = 0.02
INFO:root:Precision = 0.12
INFO:root:Recall = 0.01


convrnn4:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Input(shape=[None, 10057, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=50, kernel_size=12, strides=4,
                            kernel_regularizer=l2(alpha), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=20, kernel_size=12, strides=4,
                            kernel_regularizer=l2(alpha), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.GlobalAveragePooling1D()),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(keras.layers.Dense(100, kernel_regularizer=l2(alpha), activation='relu')),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.3135 - accuracy: 0.8910 - val_loss: 0.3298 - val_accuracy: 0.8813

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00

convrnn4:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Input(shape=[None, 10057, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=50, kernel_size=12, strides=4,
                            kernel_regularizer=l2(alpha), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.5),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=20, kernel_size=12, strides=4,
                            kernel_regularizer=l2(alpha), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.TimeDistributed(keras.layers.Flatten()),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(keras.layers.Dense(100, kernel_regularizer=l2(alpha), activation='relu')),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.1448 - accuracy: 0.9679 - val_loss: 1.0802 - val_accuracy: 0.8813

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00

Notes: model is overfitting, try adding regularization


convrnn6:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Input(shape=[None, 10057, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=50, kernel_size=12, strides=4,
                            kernel_regularizer=l2(0.2), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.5),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=20, kernel_size=12, strides=4,
                            kernel_regularizer=l2(0.2), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.TimeDistributed(keras.layers.Flatten()),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(keras.layers.Dense(100, kernel_regularizer=l2(alpha), activation='relu')),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.3955 - accuracy: 0.8975 - val_loss: 0.6179 - val_accuracy: 0.8813

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00