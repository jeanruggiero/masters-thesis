lstm1:
model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.Dense(2, activation='softmax')
])

batches = 10
n = 10
epochs = 30
patience = 10

Mean Jaccard Index = 0.07
f1-score = 0.21
Precision = 0.62
Recall = 0.10


Notes: baseline model used to troubleshoot infrastructure code. Logging facility added, required info copied to s3 after model run, fault-tolerant
data-loading, parallelized loading & preprocessing, etc. Baseline model is overfitting. Model is probably not powerful enough - try adding
additional layers.


lstm2:
model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(0.2), dropout=0.5),
    keras.layers.Dense(2, activation='softmax')
])

batches = 10
n = 10
epochs = 30
patience = 10

loss: 0.2739 - accuracy: 0.8888 - val_loss: 0.2970 - val_accuracy: 0.8813

val_size = 30,000

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00

Notes: lstm2 guesses negative class for every sample. Training and validation accuracy are approximately equal, so it's not a matter of overfitting
. Hypothesis is that the model is overly regularized. Try reducing regularization in next model.


ltsm3:
model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(0.01), dropout=0.5),
    keras.layers.Dense(2, activation='softmax')
])

Metrics after batch 8:
INFO:root:Mean Jaccard Index = 0.11
INFO:root:TP = 580
INFO:root:FP = 970
INFO:root:FN = 2980
INFO:root:f1-score = 0.29
INFO:root:Precision = 0.37
INFO:root:Recall = 0.16


Metrics after batch 9 (final batch):
INFO:root:Mean Jaccard Index = 0.01
INFO:root:TP = 43
INFO:root:FP = 49
INFO:root:FN = 3517
INFO:root:f1-score = 0.02
INFO:root:Precision = 0.47
INFO:root:Recall = 0.01


Notes: model overfits severely on first two batches (predicts all zeros). After batch 4, model is no longer predicting all zeros. This model is
overfitting again, so try increaing regularization slightly. What causes the model to get worse with additional data?

lstm4:
l2_reg = 0.05

model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.2747 - accuracy: 0.8888 - val_loss: 0.2866 - val_accuracy: 0.8813

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00


lstm5:

l2_reg = 0.03

model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(500, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(300, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(200, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.BatchNormalization(),
    keras.layers.LSTM(100, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.5),
    keras.layers.Dense(2, activation='softmax')
])

INFO:root:Mean Jaccard Index = 0.02
INFO:root:TP = 85
INFO:root:FP = 51
INFO:root:FN = 3475
INFO:root:f1-score = 0.05
INFO:root:Precision = 0.62
INFO:root:Recall = 0.02


lstm6:
l2_reg = 0.03

model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(l2_reg), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.2349 - accuracy: 0.9556 - val_loss: 0.7976 - val_accuracy: 0.8446

INFO:root:Mean Jaccard Index = 0.02
INFO:root:TP = 170
INFO:root:FP = 1271
INFO:root:FN = 3390
INFO:root:f1-score = 0.07
INFO:root:Precision = 0.12
INFO:root:Recall = 0.05

Notes: works much better than multi-layer LSTM, but overfitting. Try increasing regularization. Also try single-layer LSTM.


rnn2:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.2321 - accuracy: 0.9413 - val_loss: 0.6499 - val_accuracy: 0.8697

INFO:root:Mean Jaccard Index = 0.06
INFO:root:TP = 290
INFO:root:FP = 639
INFO:root:FN = 3270
INFO:root:f1-score = 0.15
INFO:root:Precision = 0.31
INFO:root:Recall = 0.08

Notes: increasing regularization has helped slightly. Increase regularization a little more.

rnn3:
alpha = 0.1

model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.2745 - accuracy: 0.9466 - val_loss: 0.6988 - val_accuracy: 0.8744

INFO:root:Mean Jaccard Index = 0.02
INFO:root:TP = 80
INFO:root:FP = 286
INFO:root:FN = 3480
INFO:root:f1-score = 0.04
INFO:root:Precision = 0.22
INFO:root:Recall = 0.02

Notes: could try more epochs with this particular model. Try adding dense layers before recurrent layer. Successive calls to fit will incrementally
 train the model: https://github.com/keras-team/keras/issues/4446

rnn4:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(100, kernel_regularizer=l2(alpha)),
    keras.layers.Dropout(0.2),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(200, kernel_regularizer=l2(alpha)),
    keras.layers.Dropout(0.2),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(250, kernel_regularizer=l2(alpha)),
    keras.layers.Dropout(0.2),
    keras.layers.BatchNormalization(),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.3113 - accuracy: 0.9160 - val_loss: 0.4943 - val_accuracy: 0.8813

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00


rnn5:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Masking(mask_value=0, input_shape=[None, 10057]),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(100, kernel_regularizer=l2(alpha)),
    keras.layers.Dropout(0.2),
    keras.layers.BatchNormalization(),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.2158 - accuracy: 0.9500 - val_loss: 0.4441 - val_accuracy: 0.8897

INFO:root:Mean Jaccard Index = 0.10
INFO:root:TP = 444
INFO:root:FP = 193
INFO:root:FN = 3116
INFO:root:f1-score = 0.27
INFO:root:Precision = 0.70
INFO:root:Recall = 0.12

Notes: try 1-convolution


rnn6:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Input(shape=[None, 10057, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=50, kernel_size=5, strides=10,
                        kernel_regularizer=l2(alpha), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.TimeDistributed(keras.layers.Flatten()),
    keras.layers.TimeDistributed(keras.layers.Dense(100, kernel_regularizer=l2(alpha))),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.3147 - accuracy: 0.8910 - val_loss: 0.3319 - val_accuracy: 0.8813

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00

notes: model not learning anything. Try reducing size of convolutional filters, reducing stride, and adding additional convolutional and pooling
layers.

convrnn2:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Input(shape=[None, 10057, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=50, kernel_size=10, strides=2,
                            kernel_regularizer=l2(alpha), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.TimeDistributed(keras.layers.Flatten()),
    keras.layers.TimeDistributed(keras.layers.Dense(100, kernel_regularizer=l2(alpha))),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

Results after training on 5 batches:
INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00

Notes: add batch normalization layers, add activation function to dense layer

convrnn3:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Input(shape=[None, 10057, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=50, kernel_size=10, strides=2,
                            kernel_regularizer=l2(alpha), activation='relu')
    ),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.TimeDistributed(keras.layers.Flatten()),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(keras.layers.Dense(100, kernel_regularizer=l2(alpha), activation='relu')),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

Metrics after first batch:
INFO:root:Mean Jaccard Index = 0.05
INFO:root:TP = 643
INFO:root:FP = 3295
INFO:root:FN = 2917
INFO:root:f1-score = 0.21
INFO:root:Precision = 0.16
INFO:root:Recall = 0.18

loss: 0.3924 - accuracy: 0.9288 - val_loss: 0.5722 - val_accuracy: 0.8753

INFO:root:Mean Jaccard Index = 0.02
INFO:root:TP = 30
INFO:root:FP = 210
INFO:root:FN = 3530
INFO:root:f1-score = 0.02
INFO:root:Precision = 0.12
INFO:root:Recall = 0.01


convrnn4:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Input(shape=[None, 10057, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=50, kernel_size=12, strides=4,
                            kernel_regularizer=l2(alpha), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=20, kernel_size=12, strides=4,
                            kernel_regularizer=l2(alpha), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.GlobalAveragePooling1D()),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(keras.layers.Dense(100, kernel_regularizer=l2(alpha), activation='relu')),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.3135 - accuracy: 0.8910 - val_loss: 0.3298 - val_accuracy: 0.8813

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00

convrnn4:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Input(shape=[None, 10057, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=50, kernel_size=12, strides=4,
                            kernel_regularizer=l2(alpha), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.5),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=20, kernel_size=12, strides=4,
                            kernel_regularizer=l2(alpha), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.TimeDistributed(keras.layers.Flatten()),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(keras.layers.Dense(100, kernel_regularizer=l2(alpha), activation='relu')),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.1448 - accuracy: 0.9679 - val_loss: 1.0802 - val_accuracy: 0.8813

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00

Notes: model is overfitting, try adding regularization


convrnn6:
alpha = 0.05

model = keras.models.Sequential([
    keras.layers.Input(shape=[None, 10057, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=50, kernel_size=12, strides=4,
                            kernel_regularizer=l2(0.2), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.5),
    keras.layers.TimeDistributed(
        keras.layers.Conv1D(filters=20, kernel_size=12, strides=4,
                            kernel_regularizer=l2(0.2), activation='relu')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool1D(pool_size=5, strides=2)),
    keras.layers.TimeDistributed(keras.layers.Flatten()),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(keras.layers.Dense(100, kernel_regularizer=l2(alpha), activation='relu')),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.3955 - accuracy: 0.8975 - val_loss: 0.6179 - val_accuracy: 0.8813

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00

conv2d1:

alpha = 0.05

window_size = 5

model = keras.models.Sequential([
    keras.layers.Input(shape=[None, 10057, window_size, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv2D(filters=10, kernel_size=(50, 3), strides=(20, 1),
                            kernel_regularizer=l2(0.2), activation='relu', padding='same')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool2D(pool_size=(5, 2), strides=(2, 1))),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(keras.layers.Flatten()),
    keras.layers.Dropout(0.5),
    keras.layers.TimeDistributed(keras.layers.Dense(100, kernel_regularizer=l2(alpha), activation='relu')),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

loss: 0.3250 - accuracy: 0.8853 - val_loss: 0.3456 - val_accuracy: 0.8751

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3560
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00

Notes: try increasing window size. Tried above model with window size 20, keeps getting killed.

    alpha = 0.05

    window_size = 20

    model = keras.models.Sequential([
        keras.layers.Input(shape=[None, 480, window_size, 1]),
        keras.layers.BatchNormalization(),
        keras.layers.TimeDistributed(
            keras.layers.Conv2D(filters=10, kernel_size=(5, 3), strides=(2, 1),
                                kernel_regularizer=l2(0.2), activation='relu', padding='same')
        ),
        keras.layers.TimeDistributed(keras.layers.MaxPool2D(pool_size=(5, 2), strides=(2, 1))),
        keras.layers.TimeDistributed(
            keras.layers.Conv2D(filters=10, kernel_size=(3, 3), strides=(2, 1),
                                kernel_regularizer=l2(0.2), activation='relu', padding='same')
        ),
        keras.layers.TimeDistributed(keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 1))),
        keras.layers.BatchNormalization(),
        keras.layers.TimeDistributed(keras.layers.Flatten()),
        keras.layers.Dropout(0.5),
        keras.layers.TimeDistributed(keras.layers.Dense(100, kernel_regularizer=l2(alpha), activation='relu')),
        keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
        keras.layers.Dense(2, activation='softmax')
    ])

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3552
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00


conv2d2:

alpha = 0.05
window_size = 20

model = keras.models.Sequential([
    keras.layers.Input(shape=[None, 480, window_size, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(
        keras.layers.Conv2D(filters=10, kernel_size=(5, 3), strides=(2, 1),
                            kernel_regularizer=l2(0.2), activation='relu', padding='same')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool2D(pool_size=(5, 2), strides=(2, 1))),
    keras.layers.TimeDistributed(
        keras.layers.Conv2D(filters=10, kernel_size=(3, 3), strides=(2, 1),
                            kernel_regularizer=l2(0.2), activation='relu', padding='same')
    ),
    keras.layers.TimeDistributed(keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 1))),
    keras.layers.BatchNormalization(),
    keras.layers.TimeDistributed(keras.layers.Flatten()),
    keras.layers.Dropout(0.5),
    keras.layers.TimeDistributed(keras.layers.Dense(100, kernel_regularizer=l2(alpha), activation='relu')),
    keras.layers.SimpleRNN(100, return_sequences=True, kernel_regularizer=l2(alpha), dropout=0.2),
    keras.layers.Dense(2, activation='softmax')
])

INFO:root:Mean Jaccard Index = 0.00
INFO:root:TP = 0
INFO:root:FP = 0
INFO:root:FN = 3552
INFO:root:f1-score = 0.00
INFO:root:Precision = nan
INFO:root:Recall = 0.00


Notes: seems like not enough training data for the model to learn anything.





conv1:


alpha = 0.05

model = keras.models.Sequential([
    #keras.layers.InputLayer(shape=[144, 480, 1]),
    keras.layers.BatchNormalization(),
    keras.layers.Conv2D(filters=10, kernel_size=(3, 3), strides=(1, 1), kernel_regularizer=l2(0.2),
                        activation='relu', padding='same'),
    keras.layers.MaxPool2D(pool_size=(3, 3), strides=(1, 1)),
    keras.layers.Conv2D(filters=20, kernel_size=(3, 3), strides=(1, 1), kernel_regularizer=l2(0.2),
                        activation='relu', padding='same'),
    keras.layers.Conv2D(filters=20, kernel_size=(3, 3), strides=(1, 1), kernel_regularizer=l2(0.2),
                        activation='relu', padding='same'),
    keras.layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1)),
    keras.layers.Conv2D(filters=25, kernel_size=(3, 3), strides=(1, 1), kernel_regularizer=l2(0.2),
                        activation='relu', padding='same'),
    keras.layers.Conv2D(filters=25, kernel_size=(3, 3), strides=(1, 1), kernel_regularizer=l2(0.2),
                        activation='relu', padding='same'),
    keras.layers.MaxPool2D(pool_size=(2, 2), strides=(1, 1)),
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation="relu"),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(64, activation="relu"),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(2, activation='softmax')
])



INFO:root:TP = 580
INFO:root:FP = 90
INFO:root:FN = 0
INFO:root:f1-score = 0.93
INFO:root:Precision = 0.87
INFO:root:Recall = 1.00


Results on real data:

[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1
 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1
 1 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
 1 1 1 1 0 0 0 0]
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1]
[[9.97033596e-01 2.96641164e-03]
 [9.80065823e-01 1.99341383e-02]
 [9.98428524e-01 1.57143164e-03]
 [9.99747813e-01 2.52184545e-04]
 [9.99729455e-01 2.70573044e-04]
 [9.99502778e-01 4.97245986e-04]
 [9.96547759e-01 3.45223024e-03]
 [9.78982031e-01 2.10179836e-02]
 [9.54132497e-01 4.58674841e-02]
 [9.86042798e-01 1.39572797e-02]
 [9.90947545e-01 9.05244518e-03]
 [9.67081606e-01 3.29183526e-02]
 [9.43427265e-01 5.65727539e-02]
 [9.56501067e-01 4.34989594e-02]
 [9.97103751e-01 2.89627281e-03]
 [9.99170423e-01 8.29661964e-04]
 [9.99898076e-01 1.01864243e-04]
 [2.28905542e-17 1.00000000e+00]
 [7.00421491e-19 1.00000000e+00]
 [3.21085313e-18 1.00000000e+00]
 [3.49833228e-20 1.00000000e+00]
 [1.31708188e-20 1.00000000e+00]
 [5.19737224e-19 1.00000000e+00]
 [6.26690248e-21 1.00000000e+00]
 [3.21526350e-05 9.99967813e-01]
 [2.95080572e-05 9.99970436e-01]
 [9.39017355e-01 6.09826930e-02]
 [9.99960065e-01 3.98792363e-05]
 [9.99928832e-01 7.11443936e-05]
 [9.99997854e-01 2.16747299e-06]
 [9.99997616e-01 2.39840938e-06]
 [9.97764826e-01 2.23521423e-03]
 [7.46890873e-05 9.99925256e-01]
 [5.61807640e-02 9.43819284e-01]
 [1.39020199e-06 9.99998569e-01]
 [1.34463353e-19 1.00000000e+00]
 [1.80169489e-19 1.00000000e+00]
 [2.66876280e-19 1.00000000e+00]
 [2.62693615e-19 1.00000000e+00]
 [2.45887011e-19 1.00000000e+00]
 [2.43560643e-19 1.00000000e+00]
 [2.31494042e-19 1.00000000e+00]
 [2.67120711e-19 1.00000000e+00]
 [2.67616374e-19 1.00000000e+00]
 [2.31905048e-19 1.00000000e+00]
 [2.05267687e-19 1.00000000e+00]
 [2.17851301e-19 1.00000000e+00]
 [7.42463060e-22 1.00000000e+00]
 [4.80102704e-20 1.00000000e+00]
 [1.00457125e-18 1.00000000e+00]
 [6.11624712e-20 1.00000000e+00]
 [9.89905457e-21 1.00000000e+00]
 [4.62542338e-20 1.00000000e+00]
 [1.56627069e-20 1.00000000e+00]
 [8.97887947e-20 1.00000000e+00]
 [4.86629999e-20 1.00000000e+00]
 [1.36419871e-20 1.00000000e+00]
 [1.07369231e-20 1.00000000e+00]
 [2.86582927e-20 1.00000000e+00]
 [2.97426683e-20 1.00000000e+00]
 [2.96987922e-20 1.00000000e+00]
 [6.92717945e-20 1.00000000e+00]
 [1.65825414e-20 1.00000000e+00]
 [2.12877637e-21 1.00000000e+00]
 [3.33390501e-21 1.00000000e+00]
 [4.89099850e-19 1.00000000e+00]
 [9.34614314e-20 1.00000000e+00]
 [5.97862301e-20 1.00000000e+00]
 [1.52216471e-19 1.00000000e+00]
 [6.13915485e-20 1.00000000e+00]
 [7.81659506e-20 1.00000000e+00]
 [2.84068653e-20 1.00000000e+00]
 [5.36229073e-20 1.00000000e+00]
 [5.15119941e-21 1.00000000e+00]
 [1.01979850e-20 1.00000000e+00]
 [2.93716100e-20 1.00000000e+00]
 [8.06268970e-20 1.00000000e+00]
 [1.64773619e-20 1.00000000e+00]
 [2.36891457e-20 1.00000000e+00]
 [3.20189352e-22 1.00000000e+00]
 [2.29375230e-22 1.00000000e+00]
 [5.15298534e-22 1.00000000e+00]
 [8.44404337e-22 1.00000000e+00]
 [2.27649491e-20 1.00000000e+00]
 [2.48801327e-21 1.00000000e+00]
 [5.72465227e-22 1.00000000e+00]
 [1.64908564e-23 1.00000000e+00]
 [9.18201593e-23 1.00000000e+00]
 [1.23254390e-14 1.00000000e+00]
 [2.70939451e-11 1.00000000e+00]
 [2.82286062e-14 1.00000000e+00]
 [1.61506673e-18 1.00000000e+00]
 [4.02502157e-19 1.00000000e+00]
 [1.93880093e-18 1.00000000e+00]
 [9.23032730e-18 1.00000000e+00]
 [3.81611090e-18 1.00000000e+00]
 [3.32579513e-17 1.00000000e+00]
 [5.07400158e-16 1.00000000e+00]
 [5.82494648e-15 1.00000000e+00]
 [4.68506019e-14 1.00000000e+00]
 [3.00735872e-16 1.00000000e+00]
 [3.32163606e-17 1.00000000e+00]
 [1.09636264e-17 1.00000000e+00]
 [1.90587583e-18 1.00000000e+00]
 [5.90818674e-18 1.00000000e+00]
 [1.35830108e-18 1.00000000e+00]
 [7.95837136e-19 1.00000000e+00]
 [5.44059074e-19 1.00000000e+00]
 [4.10714871e-19 1.00000000e+00]
 [2.16039623e-20 1.00000000e+00]
 [1.92773883e-18 1.00000000e+00]
 [2.96210631e-20 1.00000000e+00]
 [9.22191095e-19 1.00000000e+00]
 [1.81649780e-19 1.00000000e+00]
 [3.00105973e-21 1.00000000e+00]
 [6.87875449e-20 1.00000000e+00]
 [4.59298549e-19 1.00000000e+00]
 [1.58387685e-18 1.00000000e+00]
 [6.94852667e-23 1.00000000e+00]
 [2.27959607e-21 1.00000000e+00]
 [3.81475562e-19 1.00000000e+00]
 [6.19223650e-21 1.00000000e+00]
 [1.12434215e-24 1.00000000e+00]
 [2.81136274e-21 1.00000000e+00]
 [5.88982168e-19 1.00000000e+00]
 [1.07953225e-20 1.00000000e+00]
 [1.49497775e-22 1.00000000e+00]
 [3.63870928e-24 1.00000000e+00]
 [2.09762627e-23 1.00000000e+00]
 [4.46122641e-20 1.00000000e+00]
 [7.40810870e-22 1.00000000e+00]
 [1.07953225e-20 1.00000000e+00]
 [1.49497775e-22 1.00000000e+00]
 [3.63870928e-24 1.00000000e+00]
 [2.09762627e-23 1.00000000e+00]
 [4.46122641e-20 1.00000000e+00]
 [5.46077186e-22 1.00000000e+00]
 [1.05872628e-25 1.00000000e+00]
 [2.14932260e-21 1.00000000e+00]
 [3.86159123e-20 1.00000000e+00]
 [6.68832980e-22 1.00000000e+00]
 [1.34653703e-20 1.00000000e+00]
 [1.90919031e-22 1.00000000e+00]
 [1.29369735e-21 1.00000000e+00]
 [3.12830630e-22 1.00000000e+00]
 [6.09597629e-22 1.00000000e+00]
 [4.21993662e-21 1.00000000e+00]
 [2.60202740e-23 1.00000000e+00]
 [2.09550777e-22 1.00000000e+00]
 [5.68727869e-22 1.00000000e+00]
 [5.01856819e-21 1.00000000e+00]
 [3.38103323e-23 1.00000000e+00]
 [1.96474391e-22 1.00000000e+00]
 [1.46231677e-19 1.00000000e+00]
 [9.18535280e-21 1.00000000e+00]
 [1.65983070e-19 1.00000000e+00]
 [9.85571027e-01 1.44289136e-02]
 [9.98962760e-01 1.03721011e-03]
 [9.99814332e-01 1.85734549e-04]
 [9.99736488e-01 2.63481139e-04]
 [9.99567926e-01 4.32110275e-04]
 [9.99562562e-01 4.37478273e-04]
 [9.99564707e-01 4.35259048e-04]
 [9.99137759e-01 8.62252840e-04]
 [9.99080300e-01 9.19720682e-04]
 [9.96863246e-01 3.13682901e-03]
 [9.94536817e-01 5.46313357e-03]
 [9.98475969e-01 1.52407365e-03]
 [9.98726666e-01 1.27329619e-03]
 [9.72898090e-21 1.00000000e+00]
 [1.07142564e-20 1.00000000e+00]
 [1.42129372e-20 1.00000000e+00]
 [9.54972585e-21 1.00000000e+00]
 [7.93275093e-21 1.00000000e+00]
 [1.18596003e-20 1.00000000e+00]
 [5.71362135e-19 1.00000000e+00]
 [2.25679600e-22 1.00000000e+00]
 [2.79049113e-24 1.00000000e+00]
 [8.57143292e-20 1.00000000e+00]
 [5.78119839e-17 1.00000000e+00]
 [1.81766507e-16 1.00000000e+00]
 [8.36353136e-18 1.00000000e+00]
 [3.99184253e-18 1.00000000e+00]
 [8.44838205e-17 1.00000000e+00]
 [2.97178335e-20 1.00000000e+00]
 [2.70777024e-20 1.00000000e+00]
 [9.55061494e-18 1.00000000e+00]
 [1.64946111e-18 1.00000000e+00]
 [8.59399608e-19 1.00000000e+00]
 [6.72800082e-20 1.00000000e+00]
 [4.89067920e-20 1.00000000e+00]
 [1.91833960e-20 1.00000000e+00]
 [1.67914579e-20 1.00000000e+00]
 [5.54649675e-20 1.00000000e+00]
 [7.13100451e-20 1.00000000e+00]
 [7.51322257e-19 1.00000000e+00]
 [1.00118178e-18 1.00000000e+00]
 [8.98741662e-19 1.00000000e+00]
 [1.05429277e-18 1.00000000e+00]
 [1.01512639e-18 1.00000000e+00]
 [2.02457987e-19 1.00000000e+00]
 [3.88975738e-19 1.00000000e+00]
 [1.69922479e-19 1.00000000e+00]
 [7.96988520e-19 1.00000000e+00]
 [5.47398247e-19 1.00000000e+00]
 [1.30621856e-21 1.00000000e+00]
 [8.88425905e-21 1.00000000e+00]
 [3.49850030e-23 1.00000000e+00]
 [3.35174493e-21 1.00000000e+00]
 [3.44229764e-19 1.00000000e+00]
 [5.46607411e-19 1.00000000e+00]
 [1.62928521e-20 1.00000000e+00]
 [2.69801785e-19 1.00000000e+00]
 [2.59878900e-19 1.00000000e+00]
 [8.28553629e-19 1.00000000e+00]
 [1.00066624e-18 1.00000000e+00]
 [5.94798385e-19 1.00000000e+00]
 [6.19656377e-20 1.00000000e+00]
 [9.43994929e-20 1.00000000e+00]
 [6.50309390e-19 1.00000000e+00]
 [7.37636812e-19 1.00000000e+00]
 [7.69664289e-20 1.00000000e+00]
 [8.22493217e-15 1.00000000e+00]
 [5.33901278e-16 1.00000000e+00]
 [3.98779431e-18 1.00000000e+00]
 [4.44443109e-18 1.00000000e+00]
 [8.92869982e-21 1.00000000e+00]
 [2.46887745e-21 1.00000000e+00]
 [4.59433270e-20 1.00000000e+00]
 [1.51738180e-19 1.00000000e+00]]

Total samples: 230
Total positive: 194
Total negative: 36
INFO:root:TP = 105
INFO:root:FP = 89
INFO:root:FN = 7
f1-score = 0.69
precision = 0.54
recall = 0.94

